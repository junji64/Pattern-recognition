{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 이차분류기(Quadratic Discriminant)\n",
    "\n",
    "가우시안 분포의 공분산행렬의 종류에 따라 여러 형태가 존재\n",
    "\n",
    "Case 1: 여기에서 i 는 서로 다른 클래스를 말함. \n",
    "1. 클래스들의 분산이 모두 같으며 \n",
    "2. 공분산행렬이 대각이며 \n",
    "3. 모든 방향으로의 분산이 같은 경우.\n",
    "$$ \\Sigma_i = \\sigma^2 I = \\begin{bmatrix} \\sigma^2 & 0 & 0 \\\\  0 & \\sigma^2 & 0  \\\\ 0 & 0 & \\sigma^2   \\end{bmatrix} $$ \n",
    "\n",
    "Case 2: 여기에서 i 는 서로 다른 클래스를 말함. \n",
    "1. 클래스들의 분산이 모두 같으며 \n",
    "2. 공분산행렬이 대각이며 \n",
    "3. 모든 방향으로의 분산이 같지는 않은 경우\n",
    "$$ \\Sigma_i = \\Sigma = \\begin{bmatrix} \\sigma_1^2 & 0 & 0 \\\\  0 & \\sigma_2^2 & 0  \\\\ 0 & 0 & \\sigma_3^2   \\end{bmatrix} $$ \n",
    "\n",
    "Case 3: 여기에서 i 는 서로 다른 클래스를 말함. \n",
    "1. 클래스들의 분산이 모두 같지만 \n",
    "2. 공분산행렬이 비대각인 경우\n",
    "$$ \\Sigma_i = \\Sigma = \\begin{bmatrix} \\sigma_1^2 & c_{12} & c_{13} \\\\  c_{12} & \\sigma_2^2 & c_{23}  \\\\ c_{13} & c_{23} & \\sigma_3^2   \\end{bmatrix} $$ \n",
    "\n",
    "Case 4: 여기에서 i 는 서로 다른 클래스를 말함. \n",
    "1. 클래스들의 분산이 모두 다르지만\n",
    "2. 공분산행렬이 대각인 경우\n",
    "$$ \\Sigma_i = \\sigma_i^2 I = \\begin{bmatrix} \\sigma_i^2 & 0 & 0 \\\\  0 & \\sigma_i^2 & 0  \\\\ 0 & 0 & \\sigma_i^2   \\end{bmatrix} $$ \n",
    "\n",
    "Case 5: 일반형\n",
    "1. 클래스들의 분산이 모두 다르며\n",
    "2. 공분산행렬이 비대각인 경우\n",
    "$$ \\Sigma_i \\ne \\Sigma_j  $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  베이즈 이차 판별식\n",
    "\n",
    "특징벡터의 크기가 D차원인 경우, 가우시안 확률밀도 함수의 일반식\n",
    "\n",
    "$$ f_X(\\mathbf{x}) = { 1 \\over (2\\pi)^{D/2} | \\Sigma |^{1/2} } \\exp \\left[-{1 \\over 2} (\\mathbf{x} -\\mathbf{\\mu})^T \\Sigma^{-1} (\\mathbf{x}-\\mathbf{\\mu}) \\right]$$\n",
    "\n",
    "위의 식으로부터 MAP(Maximum A Posteriori: 최대사후확률) 판별함수는 다음과 같이 유도된다.\n",
    "\n",
    "$$ g_i(\\mathbf{x}) = P(\\omega_i | \\mathbf{x}) = { P(\\mathbf{x}|\\omega_i) P(\\omega_i) \\over P(\\mathbf{x}) }$$\n",
    "$$= { 1 \\over (2\\pi)^{D/2} | \\Sigma |^{1/2} } \\exp \\left[-{1 \\over 2} (\\mathbf{x} -\\mathbf{\\mu})^T \\Sigma^{-1} (\\mathbf{x}-\\mathbf{\\mu}) \\right] {P(\\omega_i) \\over P(\\mathbf{x})} $$\n",
    "\n",
    "상수 항들($P(\\mathbf{x}), 2\\pi$)을 제거하고 다시 쓰면\n",
    "\n",
    "$$ g_i(\\mathbf{x}) = | \\Sigma |^{-1/2} \\exp \\left[-{1 \\over 2} (\\mathbf{x} -\\mathbf{\\mu})^T \\Sigma^{-1} (\\mathbf{x}-\\mathbf{\\mu}) \\right] P(\\omega_i) $$\n",
    "\n",
    "위 식에 단조증가함수인 자연로그를 취하면\n",
    "\n",
    "$$ g_i(\\mathbf{x}) = -{1 \\over 2} (\\mathbf{x} -\\mathbf{\\mu})^T \\Sigma^{-1} (\\mathbf{x}-\\mathbf{\\mu})\n",
    "- {1 \\over 2} \\log(|\\Sigma_i|) + \\log (P(\\omega_i)) $$\n",
    "\n",
    "이 되고, 두번째 세번째 항은 실수의 스칼라 값이 되므로,\n",
    "\n",
    "$$ g_i(\\mathbf{x}) = ax_1^2 + bx_2^2 + \\cdots + cx_D^2 + dx_1 x_2 + ex_1x_3 + \\cdots + fx_{D-1}x_D + g x_1+ h x_2 + \\cdots + ix_D + j$$\n",
    "\n",
    "$\\Rightarrow$ 베이즈의 이차판별 함수식\n",
    "\n",
    "$$\n",
    "\\because (\\mathbf{x} -\\mathbf{\\mu})^T \\Sigma^{-1} (\\mathbf{x}-\\mathbf{\\mu}) = \n",
    "\\begin{bmatrix} x_1-\\mu_1\\\\ x_2-\\mu_2\\\\ \\vdots \\\\ x_D-\\mu_D  \\end{bmatrix}^T\n",
    " \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1D} \\\\ a_{12} & a_{22} & \\cdots & a_{2D} \\\\ \\vdots &  & \\cdots & \\vdots \\\\ a_{1D} & \\cdots & \\cdots & a_{DD} \\end{bmatrix} \\begin{bmatrix} x_1-\\mu_1\\\\x_2-\\mu_2\\\\ \\vdots \\\\ x_D-\\mu_D  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    " = \n",
    "\\begin{bmatrix} a_{11}(x_1-\\mu_1) + a_{12}(x_2-\\mu_2) + \\vdots + a_{1D}(x_D-\\mu_D) \\\\\n",
    "a_{12}(x_1-\\mu_1) + a_{22}(x_2-\\mu_2) + \\vdots + a_{2D}(x_D-\\mu_D) \\\\\n",
    "\\vdots \\\\\n",
    "a_{1D}(x_1-\\mu_1) + a_{2D}(x_2-\\mu_2) + \\vdots + a_{DD}(x_D-\\mu_D) \\\\\n",
    "\\end{bmatrix}^T \\begin{bmatrix} x_1-\\mu_1\\\\x_2-\\mu_2\\\\ \\vdots \\\\ x_D-\\mu_D  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "= ax_1^2 + bx_2^2 + \\cdots + cx_D^2 + dx_1 x_2 + ex_1x_3 + \\cdots + fx_{D-1}x_D + g x_1+ h x_2 + \\cdots + ix_D + j\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이차분류기 : Case 1 : $\\Sigma_i = \\sigma^2 I$\n",
    "\n",
    "$$ \\Sigma_i = \\sigma^2 I = \\begin{bmatrix} \\sigma^2 & 0 & 0 \\\\  0 & \\sigma^2 & 0  \\\\ 0 & 0 & \\sigma^2   \\end{bmatrix} $$\n",
    "\n",
    "Case 1: 특징들이 서로 독립적이면서, 각 클래스들의 분산이 모두 같은 경우이다.\n",
    "\n",
    "\n",
    "$$ g_i(\\mathbf{x}) = -{1 \\over 2} (\\mathbf{x} -\\mathbf{\\mu_i})^T (\\sigma^2 I)^{-1} (\\mathbf{x}-\\mathbf{\\mu_i}) - { 1 \\over 2} \\log(|\\sigma^2 I|) + \\log(P(\\omega_i)) $$\n",
    "$$ = -{1 \\over 2\\sigma^2} (\\mathbf{x} -\\mathbf{\\mu_i})^T(\\mathbf{x}-\\mathbf{\\mu_i}) - { 1 \\over 2} D\\log(\\sigma^2) + \\log(P(\\omega_i)) $$\n",
    "\n",
    "여기에서 모든 클래스, $i$,에 대하여 상수인 두번째 항을 제거하면,\n",
    "\n",
    "$$ =  -{1 \\over 2\\sigma^2} (\\mathbf{x} -\\mathbf{\\mu_i})^T(\\mathbf{x}-\\mathbf{\\mu_i})  + \\log(P(\\omega_i)) $$\n",
    "$$ = -{1 \\over 2\\sigma^2} (\\mathbf{x}^T \\mathbf{x} - 2\\mu_i^T\\mathbf{x} + \\mu_i^T \\mu_i)  + \\log(P(\\omega_i)) $$\n",
    "\n",
    "모든 클래스, $i$ 에 대하여 상수인 특징벡터의 내적 $\\mathbf{x}^T\\mathbf{x}$ 를 제거하면\n",
    "\n",
    "$$  g_i(\\mathbf{x}) = -{1 \\over 2\\sigma^2} (- 2\\mu_i^T\\mathbf{x} + \\mu_i^T \\mu_i)  + \\log(P(\\omega_i)) = w_i^T \\mathbf{x} + w_{i0}$$\n",
    "여기에서\n",
    "\\begin{cases}\n",
    "    w_i      & = { \\mu_i \\over \\sigma^2 } \\\\\n",
    "    w_{i0}  & = -{ 1 \\over 2 \\sigma^2 } \\mu_i^T\\mu_i + \\log(P(\\omega_i))\n",
    "\\end{cases}\n",
    "\n",
    "사전확률 $\\log(P(\\omega_i))$ 이 같다고 하면,\n",
    "$$ g_i(\\mathbf{x}) = -{1 \\over 2\\sigma^2} (- 2\\mu_i^T\\mathbf{x} + \\mu_i^T \\mu_i)  $$\n",
    "\n",
    "따라서, 판별함수는 $\\mathbf{x}$ 와 평균들 간의 **유클리디안 거리**함수\n",
    "\n",
    "<img src=\"images/case1.png\" width=\"400\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음과 간은 평균과 공분산을 갖는 3클래스 2차원 문제의 결정경계를 구해보자.\n",
    "\n",
    "$$ \\mu_1 = \\begin{bmatrix} 3 \\\\  2   \\end{bmatrix} , \\quad \\Sigma_1 = \\begin{bmatrix} 2 & 0 \\\\  0 & 2    \\end{bmatrix} $$\n",
    "$$ \\mu_2 = \\begin{bmatrix} 7 \\\\  4   \\end{bmatrix} , \\quad \\Sigma_2 = \\begin{bmatrix} 2 & 0 \\\\  0 & 2    \\end{bmatrix} $$\n",
    "$$ \\mu_3 = \\begin{bmatrix} 2 \\\\  5   \\end{bmatrix} , \\quad \\Sigma_3 = \\begin{bmatrix} 2 & 0 \\\\  0 & 2    \\end{bmatrix} $$\n",
    "\n",
    "<img src=\"images/case1-ex2.jpg\" width=\"400\">\n",
    "<img src=\"images/case1-ex1.jpg\"  align=\"left\"  width=\"300\">\n",
    "<img src=\"images/case1-ex3.png\"  width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이차분류기 : Case 2 : $\\Sigma_i = \\Sigma$ (Diagonal Matrix)\n",
    "\n",
    "Case 2 : 클래스들의 분산이 모두 같지만, 각 특징들의 분산은 서로 다른 경우이다.\n",
    "\n",
    "$$ \\Sigma_i = \\Sigma = \\begin{bmatrix} \\sigma_1^2 & 0 & 0 \\\\  0 & \\sigma_2^2 & 0  \\\\ 0 & 0 & \\sigma_D^2   \\end{bmatrix} $$ \n",
    "\n",
    "\n",
    "$$ g_i(\\mathbf{x}) = -{1 \\over 2} (\\mathbf{x}-\\mathbf{\\mu_i})^T \\Sigma_i^{-1}(\\mathbf{x}-\\mathbf{\\mu_i}) \n",
    "- { 1 \\over 2} \\log(|\\Sigma_i|) + \\log(P(\\omega_i)) $$\n",
    "\n",
    "\n",
    "$$ = -{1 \\over 2} (\\mathbf{x}-\\mathbf{\\mu_i})^T \\begin{bmatrix} \\sigma_1^{-2} & 0 & 0 \\\\  0 & \\sigma_2^{-2} & 0  \\\\ 0 & 0 & \\sigma_D^{-2}   \\end{bmatrix}(\\mathbf{x}-\\mathbf{\\mu_i}) \n",
    "- { 1 \\over 2} \\log( \\sigma_1^2 \\cdots \\sigma_D^2)  + \\log(P(\\omega_i)) $$\n",
    "\n",
    "\n",
    "$$ =  -{1 \\over 2} \\sum_{k=1}^D {(\\mathbf{x}[k] -\\mathbf{\\mu_i}[k])^2 \\over \\sigma_k^2 }  - { 1 \\over 2} \\log \\prod_{k=1}^D \\sigma_k^2  + \\log(P(\\omega_i)) $$\n",
    "\n",
    "\n",
    "$$ =  -{1 \\over 2} \\sum_{k=1}^D {x[k]^2 - 2x[k]\\mu_i[k] + \\mu_i[k]^2 \\over \\sigma_k^2 }  - { 1 \\over 2} \\log \\prod_{k=1}^D \\sigma_k^2  + \\log(P(\\omega_i)) $$\n",
    "\n",
    "모든 클래스, $i$,에 대하여 상수인  $x[k]^2$ 과 $ {1 \\over 2} \\log \\prod_{k=1}^D\\sigma_k^2$ 를 제거하면\n",
    "\n",
    "$$  g_i(\\mathbf{x}) =  {1 \\over 2} \\sum_{k=1}^D { 2x[k]\\mu_i[k] - \\mu_i[k]^2 \\over \\sigma_k^2 } + \\log(P(\\omega_i)) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음과 같은 평균과 공분산을 갖는 3클래스 2차원 문제의 결정경계를 구해보자.\n",
    "\n",
    "$$ \\mu_1 = \\begin{bmatrix} 3 \\\\  2   \\end{bmatrix} , \\quad \\Sigma_1 = \\begin{bmatrix} 1 & 0 \\\\  0 & 2    \\end{bmatrix} $$\n",
    "$$ \\mu_2 = \\begin{bmatrix} 5 \\\\  4   \\end{bmatrix} , \\quad \\Sigma_2 = \\begin{bmatrix} 1 & 0 \\\\  0 & 2    \\end{bmatrix} $$\n",
    "$$ \\mu_3 = \\begin{bmatrix} 2 \\\\  5   \\end{bmatrix} , \\quad \\Sigma_3 = \\begin{bmatrix} 1 & 0 \\\\  0 & 2    \\end{bmatrix} $$\n",
    "\n",
    "<img src=\"images/case2-ex1.jpg\" width=\"400\">\n",
    "<img src=\"images/case2-ex2.png\"  align=\"left\"  width=\"250\">\n",
    "<img src=\"images/case2-ex3.png\"  width=\"270\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이차분류기 : Case 3 : $\\Sigma_i = \\Sigma$ (non-Diagonal Matrix)\n",
    "\n",
    "Case 3 : 클래스들의 분산이 모두 같지만, 분산이 비대각인 경우이다.\n",
    "\n",
    "$$ \\Sigma_i = \\Sigma = \\begin{bmatrix} \\sigma_1^2 & c_{12} & c_{13} \\\\  c_{12} & \\sigma_2^2 & c_{23}  \\\\ c_{13} & c_{23} & \\sigma_3^2   \\end{bmatrix} $$ \n",
    "\n",
    "$$ g_i(\\mathbf{x})  = -{1 \\over 2} (\\mathbf{x}-\\mathbf{\\mu_i})^T \\Sigma_i^{-1}(\\mathbf{x}-\\mathbf{\\mu_i}) \n",
    "- { 1 \\over 2} \\log(|\\Sigma_i|) + \\log(P(\\omega_i)) $$\n",
    "\n",
    "$$  = -{1 \\over 2} (\\mathbf{x}-\\mathbf{\\mu_i})^T \\Sigma^{-1}(\\mathbf{x}-\\mathbf{\\mu_i}) \n",
    "- { 1 \\over 2} \\log(|\\Sigma|) + \\log(P(\\omega_i)) $$\n",
    "\n",
    "모든 클래스 $\\omega_i$ 에 대하여 상수인  $ \\log(|\\Sigma|)$ 를 제거하면\n",
    "$$ g_i(\\mathbf{x})  = -{1 \\over 2} (\\mathbf{x}-\\mathbf{\\mu_i})^T \\Sigma_i^{-1}(\\mathbf{x}-\\mathbf{\\mu_i}) \n",
    " + \\log(P(\\omega_i)) $$\n",
    "\n",
    "판별함수는 $\\mathbf{x}$ 와 평균들 간의 **마할라노비스 거리(mahalanobis distance)** 함수 \n",
    "\n",
    "$$ ||\\mathbf{x-y}||_{\\Sigma^{-1}}^2 = (\\mathbf{x-y})^T\\Sigma^{-1}(\\mathbf{x-y})$$\n",
    "\n",
    "<img src=\"images/mahalanobis.png\"  width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음과 같은 평균과 공분산을 갖는 3클래스 2차원 문제의 결정경계를 구해보자.\n",
    "\n",
    "$$ \\mu_1 = \\begin{bmatrix} 3 \\\\  2   \\end{bmatrix} , \\quad \\Sigma_1 = \\begin{bmatrix} 1 & 0.7 \\\\  0 .7 & 2    \\end{bmatrix} $$\n",
    "$$ \\mu_2 = \\begin{bmatrix} 5 \\\\  4   \\end{bmatrix} , \\quad \\Sigma_2 = \\begin{bmatrix} 1 & 0.7 \\\\  0.7 & 2    \\end{bmatrix} $$\n",
    "$$ \\mu_3 = \\begin{bmatrix} 2 \\\\  5   \\end{bmatrix} , \\quad \\Sigma_3 = \\begin{bmatrix} 1 & 0.7 \\\\  0.7 & 2    \\end{bmatrix} $$\n",
    "\n",
    "<img src=\"images/case3-ex1.jpg\" width=\"400\">\n",
    "<img src=\"images/case3-ex2.jpg\"  align=\"left\"  width=\"250\">\n",
    "<img src=\"images/case3-ex3.png\"  width=\"270\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이차분류기 : Case 4 : $\\Sigma_i = \\sigma_i^2 I$\n",
    "\n",
    "Case 4 : 각 클래스들의 분산은 서로 다르면서  대각인 경우이다. \n",
    "\n",
    "$$ \\Sigma_i = \\sigma_i^2 I = \\begin{bmatrix} \\sigma_i^2 & 0 & 0 \\\\ 0 & \\sigma_i^2 &0  \\\\ 0 & 0 & \\sigma_i^2   \\end{bmatrix} $$ \n",
    "\n",
    "\n",
    "$$ g_i(\\mathbf{x})  = -{1 \\over 2} (\\mathbf{x}-\\mathbf{\\mu_i})^T \\Sigma_i^{-1}(\\mathbf{x}-\\mathbf{\\mu_i}) \n",
    "- { 1 \\over 2} \\log(|\\Sigma_i|) + \\log(P(\\omega_i)) $$\n",
    "\n",
    "\n",
    "$$  = -{1 \\over 2} (\\mathbf{x}-\\mathbf{\\mu_i})^T \\sigma_i^2(\\mathbf{x}- \\mathbf{\\mu_i}) \n",
    "- { 1 \\over 2} D \\log(\\sigma_i^2) + \\log(P(\\omega_i)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음과 같은 평균과 공분산을 갖는 3클래스 2차원 문제의 결정경계를 구해보자.\n",
    "\n",
    "$$ \\mu_1 = \\begin{bmatrix} 3 \\\\  2   \\end{bmatrix} , \\quad \\Sigma_1 = \\begin{bmatrix} 0.5 & 0  \\\\  0   & 0.5    \\end{bmatrix} $$\n",
    "$$ \\mu_2 = \\begin{bmatrix} 5 \\\\  4   \\end{bmatrix} , \\quad \\Sigma_2 = \\begin{bmatrix} 1 & 0  \\\\  0  & 1    \\end{bmatrix} $$\n",
    "$$ \\mu_3 = \\begin{bmatrix} 2 \\\\  5   \\end{bmatrix} , \\quad \\Sigma_3 = \\begin{bmatrix} 2 & 0  \\\\  0  & 2    \\end{bmatrix} $$\n",
    "\n",
    "<img src=\"images/case4-ex1.jpg\" width=\"400\">\n",
    "<img src=\"images/case4-ex2.jpg\"  align=\"left\"  width=\"250\">\n",
    "<img src=\"images/case4-ex3.png\"  width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이차분류기 : Case 5 : $\\Sigma_i \\ne \\Sigma_j$ (general case)\n",
    "\n",
    "Case 5 :  $\\Sigma_i \\ne \\Sigma_j$ 일반형\n",
    "\n",
    "$$g_i(\\mathbf{x})  = -{1 \\over 2} (\\mathbf{x}-\\mathbf{\\mu_i})^T \\Sigma_i^{-1}(\\mathbf{x}-\\mathbf{\\mu_i}) \n",
    "- { 1 \\over 2} \\log(|\\Sigma_i|) + \\log(P(\\omega_i)) $$\n",
    "\n",
    "$$  = \\mathbf{x}^T W_i \\mathbf{x} + w_i^T \\mathbf{x} + w_{i0} $$\n",
    "\n",
    "Where\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "   W_i &  = -{1 \\over 2} \\Sigma_i^{-1} \\\\\n",
    "   w_i & = \\Sigma_i^{-1} \\mu_i \\\\\n",
    "   w_{i0} & = -{1 \\over 2} \\mathbf{\\mu_i}^T \\Sigma_i^{-1}\\mathbf{\\mu_i} - { 1 \\over 2} \\log(|\\Sigma_i|) + \\log(P(\\omega_i) \n",
    "\\end{cases}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음과 같은 평균과 공분산을 갖는 3클래스 2차원 문제의 결정경계를 구해보자.\n",
    "\n",
    "$$ \\mu_1 = \\begin{bmatrix} 3 \\\\  2   \\end{bmatrix} , \\quad \\Sigma_1 = \\begin{bmatrix} 1 &-1  \\\\  -1   & 2    \\end{bmatrix} $$\n",
    "$$ \\mu_2 = \\begin{bmatrix} 5 \\\\  4   \\end{bmatrix} , \\quad \\Sigma_2 = \\begin{bmatrix} 1 & -1  \\\\ -1  &7    \\end{bmatrix} $$\n",
    "$$ \\mu_3 = \\begin{bmatrix} 2 \\\\  5   \\end{bmatrix} , \\quad \\Sigma_3 = \\begin{bmatrix} 0.5 & 0.5  \\\\  0.5  & 3    \\end{bmatrix} $$\n",
    "\n",
    "<img src=\"images/case5-ex1.jpg\" width=\"400\">\n",
    "<img src=\"images/case5-ex2.jpg\"  align=\"left\"  width=\"250\">\n",
    "<img src=\"images/case5-ex3.png\"  width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**베이즈 분류기는 이차분류기이다**\n",
    "\n",
    "**첫째**, 클래스들이 Case 3\n",
    "1. 모두 가우시안 분포에 따르고 \n",
    "2. 동일한 일반적 공분산 값을 가지며 \n",
    "3. 사전 확률이 같을 경우 \n",
    " “**최소-마할라노비스 (mahalanobis) 거리** 분류기” \n",
    " $$ g_i(\\mathbf{x}) = -{1 \\over 2} (\\mathbf{x} -\\mu_i)^T \\Sigma_i^{-1} (\\mathbf{x} - \\mu_i) $$\n",
    " \n",
    " \n",
    "**둘째**, 클래스들이 Case 1\n",
    "1. 모두 가우시안 분포에 따르고 \n",
    "2. 동일한 항등 행렬에 비례하는 공분산 값을 가지며 \n",
    "3. 사전 확률이 같을 경우\n",
    "“**최소-유클리디안 (euclidian) 거리** 분류기”\n",
    "$$ g_i(\\mathbf{x}) = -{1 \\over 2 \\sigma^2} (\\mathbf{x} -\\mu_i)^T (\\mathbf{x} - \\mu_i) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exmple\n",
    "\n",
    "3차원 특징 벡터 집합으로부터 정의 되는 다음과 같은 가우시안 파라미터로 이루어진 클래스 $\\omega_1, \\omega_2$에 대한 선형판별함수를 유도하고 특징벡터 $x=[0.1,  0.7, 0.8]^T$ 가 어느 클래스에 속하는 지 결정하시오.                      \n",
    "\n",
    "$$ \\mu_1 = \\begin{bmatrix} 0\\\\0\\\\0  \\end{bmatrix}; \\quad \\mu_2 = \\begin{bmatrix}1\\\\1\\\\1  \\end{bmatrix}; \\quad \\Sigma_1 = \\Sigma_2 = \\begin{bmatrix} 1/4 & 0 & 0  \\\\ 0 & 1/4 & 0 \\\\0 & 0 & 1/4 \\end{bmatrix}; \\quad p(\\omega_2) = 2*p(\\omega_1) $$\n",
    "\n",
    "(ANS)\n",
    "\n",
    "$$ g_i(\\mathbf{x}) = -{1 \\over 2} (\\mathbf{x}-\\mathbf{\\mu_i})^T \\Sigma_i^{-1}(\\mathbf{x}-\\mathbf{\\mu_i}) \n",
    " + \\ln P(\\omega_i) = { 1 \\over 2} \\begin{bmatrix} x_1-\\mu_{i1}\\\\x_2-\\mu_{i2}\\\\x_3-\\mu_{i3}  \\end{bmatrix}^T\n",
    " \\begin{bmatrix} 4 & 0 & 0  \\\\ 0 & 4 & 0 \\\\0 & 0 & 4 \\end{bmatrix} \\begin{bmatrix} x_1-\\mu_{i1}\\\\x_2-\\mu_{i2}\\\\x_3-\\mu_{i3}  \\end{bmatrix} +  \\ln P(\\omega_i)$$\n",
    " \n",
    " \n",
    "$$ g_1(x) = { 1 \\over 2} \\begin{bmatrix} x_1 \\\\x_2 \\\\x_3   \\end{bmatrix}^T\n",
    " \\begin{bmatrix} 4 & 0 & 0  \\\\ 0 & 4 & 0 \\\\0 & 0 & 4 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\x_2\\\\x_3  \\end{bmatrix} +  \\ln {1 \\over 3}$$\n",
    " \n",
    " \n",
    "$$ g_2(\\mathbf{x}) = { 1 \\over 2} \\begin{bmatrix} x_1-1\\\\x_2-1\\\\x_3-1 \\end{bmatrix}^T\n",
    " \\begin{bmatrix} 4 & 0 & 0  \\\\ 0 & 4 & 0 \\\\0 & 0 & 4 \\end{bmatrix} \\begin{bmatrix} x_1-1\\\\x_2-1\\\\x_3-1 \\end{bmatrix} +  \\ln {2 \\over 3}$$\n",
    " \n",
    " \n",
    " $$g_1(\\mathbf{x}) \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} g_2(\\mathbf{x}) $$\n",
    " \n",
    " \n",
    " $$ -2(x_1^2 +x_2^2 + x_3^2) + ln {1 \\over 3} \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}}  -2((x_1-1)^2 +(x_2-1)^2 +(x_3-1)^2 ) +ln {2 \\over 3}$$\n",
    " \n",
    " \n",
    " $$ x_1 + x_2 + x_3 \\overset{\\omega_1}{\\underset{\\omega_2}{\\lessgtr}} { 6 - ln2 \\over 4 } = 1.32 $$\n",
    " \n",
    " \n",
    " $$ 0.1 + 0.7 + 0.8 = 1.6   \\overset{\\omega_1}{\\underset{\\omega_2}{\\lessgtr}}  1.32 \\Rightarrow x_u \\in \\omega_2 $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
